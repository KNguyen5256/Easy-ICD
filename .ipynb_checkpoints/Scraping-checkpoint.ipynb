{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f60f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flickrapi\n",
    "import requests\n",
    "import io\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "from flickrapi import shorturl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f7bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"91c796378356002c5ba8be27758cada5\"\n",
    "secret = \"fd4f5ab352fa08e8\"\n",
    "\n",
    "url_string = \"https://live.staticflickr.com/{}/{}_{}_{}.jpg\"\n",
    "\n",
    "flickr = flickrapi.FlickrAPI(key, secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c532081",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "already exists!\n",
      "Saved 0 more images of class blueberry cheesecake\n",
      "104.54348874092102\n"
     ]
    }
   ],
   "source": [
    "# search_terms = ['donuts', 'pizza', 'sandwich', 'cupcake', 'croissant']\n",
    "# search_terms += ['apple_pie', 'banana bread', 'chocolate chip cookie', 'garlic bread', 'quiche']\n",
    "search_terms = ['blueberry cheesecake', 'red velvet cake']\n",
    "# search_terms += ['soda can', 'latte', '']\n",
    "\n",
    "# search_terms = ['huevos rancheros', 'taco', 'burrito', 'tamales', 'quesadilla', 'enchiladas', 'nachos']\n",
    "\n",
    "num_samples_per_class = 500\n",
    "img_size = (128, 128)\n",
    "\n",
    "size = \"m\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for search_term in search_terms[:1]:\n",
    "    curr_dir = \"./images/baked_foods/{}\".format(search_term.replace(\" \", \"_\"))\n",
    "    \n",
    "    existing_img_paths = [i for i in os.listdir(curr_dir) if \".jpg\" in i]\n",
    "    \n",
    "    if not os.path.exists(curr_dir):\n",
    "        os.mkdir(curr_dir)\n",
    "        \n",
    "    curr_photos = flickr.walk(text=search_term, media=\"photos\", sort=\"relevance\", per_page=500)\n",
    "    \n",
    "    curr_photos = itertools.islice(curr_photos, 6500, None)\n",
    "    \n",
    "    img_names = []\n",
    "    \n",
    "    for i in range(num_samples_per_class):\n",
    "        curr_photo = next(iter(curr_photos))\n",
    "\n",
    "        curr_secret = curr_photo.get('secret')\n",
    "        curr_server_id = curr_photo.get('server')\n",
    "        curr_id = curr_photo.get('id')\n",
    "\n",
    "        curr_url = url_string.format(curr_server_id, curr_id, curr_secret, size)\n",
    "\n",
    "        response = requests.get(curr_url)\n",
    "\n",
    "        image_file = io.BytesIO(response.content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        image.thumbnail(img_size)\n",
    "        image = ImageOps.pad(image, img_size)\n",
    "\n",
    "        file_path = os.path.join(curr_dir, curr_server_id + curr_id + curr_secret + '.jpg')\n",
    "\n",
    "        if curr_server_id + curr_id + curr_secret + '.jpg' not in existing_img_paths:\n",
    "            img_names.append(curr_server_id + curr_id + curr_secret + '.jpg')\n",
    "            \n",
    "            with open(file_path, 'wb') as f:\n",
    "                image.save(f, \"JPEG\", quality=85)\n",
    "        else:\n",
    "            print('already exists!')\n",
    "            \n",
    "    print(\"Saved {} more images of class {}\".format(len(set(img_names)), search_term))\n",
    "        \n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b26f60a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4309\n",
      "4229\n",
      "4453\n",
      "4351\n",
      "4514\n",
      "4202\n",
      "4100\n",
      "4438\n",
      "4178\n",
      "4215\n",
      "3918\n",
      "4183\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "search_terms = ['donuts', 'pizza', 'sandwich', 'cupcake', 'croissant']\n",
    "search_terms += ['apple_pie', 'banana bread', 'chocolate chip cookie', 'garlic bread', 'quiche']\n",
    "search_terms += ['blueberry cheesecake', 'red velvet cake']\n",
    "\n",
    "dataset_dir = \"./images/baked_foods/\"\n",
    "\n",
    "for c in range(len(search_terms)):\n",
    "    curr_class = search_terms[c].replace(\" \", \"_\")\n",
    "\n",
    "    image_paths = [i for i in os.listdir(dataset_dir + curr_class) if \".jpg\" in i]\n",
    "    \n",
    "    print(len(image_paths))\n",
    "    \n",
    "    for i in image_paths[:500]:\n",
    "        img = Image.open(dataset_dir + curr_class + '/' + i)\n",
    "        images.append(np.array(img))\n",
    "        labels.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ebac0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 128, 128, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = np.random.permutation(len(images))\n",
    "\n",
    "images = np.array(images).astype('float32')[perm]\n",
    "labels = np.array(labels)[perm]\n",
    "\n",
    "images /= 255\n",
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a3e63",
   "metadata": {},
   "source": [
    "## Define tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bef762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        \n",
    "        self.features = None\n",
    "        self.store_features = False\n",
    "        \n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                            padding=\"same\", input_shape=(128, 128, 3))\n",
    "        self.conv2 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                            padding=\"same\")\n",
    "        self.pool1 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")\n",
    "        \n",
    "        self.conv3 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                            padding=\"same\")\n",
    "        self.conv4 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                            padding=\"same\")\n",
    "        self.pool2 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")\n",
    "        \n",
    "        self.conv5 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")\n",
    "        self.conv6 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")\n",
    "        self.pool3 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")\n",
    "\n",
    "        self.conv7 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")\n",
    "        self.conv8 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")\n",
    "        self.pool4 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")\n",
    "        \n",
    "        self.conv9 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")\n",
    "        self.conv10 = Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")\n",
    "        self.pool5 = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.dense1 = Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.dense2 = Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.dense3 = Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.dense4 = Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.dense5 = Dense(units=num_classes)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        \n",
    "        if self.store_features:\n",
    "            self.features = np.copy(x)\n",
    "        \n",
    "        x = self.dense5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8511d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"simple_conv_net_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          multiple                  896       \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  multiple                 0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            multiple                  65664     \n",
      "                                                                 \n",
      " dense_16 (Dense)            multiple                  16512     \n",
      "                                                                 \n",
      " dense_17 (Dense)            multiple                  16512     \n",
      "                                                                 \n",
      " dense_18 (Dense)            multiple                  8256      \n",
      "                                                                 \n",
      " dense_19 (Dense)            multiple                  780       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 191,852\n",
      "Trainable params: 191,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SimpleConvNet(12)\n",
    "out = model(images[0:1])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d55e7",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "050dae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f41133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)\n",
    "        \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_fn(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50d4452b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8557563\n",
      "2.5488932\n",
      "2.5031066\n",
      "2.4962058\n",
      "2.4925048\n",
      "2.4904237\n",
      "2.4903033\n",
      "2.489595\n",
      "Epoch 1, Loss: 2.488921642303467, Accuracy: 8.083333015441895, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.4344368\n",
      "2.4712179\n",
      "2.4577622\n",
      "2.4525921\n",
      "2.440271\n",
      "2.4266949\n",
      "2.4197054\n",
      "2.4088383\n",
      "Epoch 2, Loss: 2.396287441253662, Accuracy: 15.050000190734863, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.6918616\n",
      "2.3995342\n",
      "2.3601682\n",
      "2.3425176\n",
      "2.3272526\n",
      "2.3267043\n",
      "2.3208802\n",
      "2.3175378\n",
      "Epoch 3, Loss: 2.31325101852417, Accuracy: 18.383333206176758, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.3099663\n",
      "2.2390382\n",
      "2.2349732\n",
      "2.2432373\n",
      "2.2476728\n",
      "2.2506545\n",
      "2.2541327\n",
      "2.2609768\n",
      "Epoch 4, Loss: 2.2637939453125, Accuracy: 20.516666412353516, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.3423839\n",
      "2.2897778\n",
      "2.2487159\n",
      "2.237496\n",
      "2.2320871\n",
      "2.228039\n",
      "2.2235944\n",
      "2.2232952\n",
      "Epoch 5, Loss: 2.222277879714966, Accuracy: 22.600000381469727, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.2237504\n",
      "2.1468716\n",
      "2.191253\n",
      "2.1953678\n",
      "2.1888282\n",
      "2.18944\n",
      "2.1819265\n",
      "2.1818602\n",
      "Epoch 6, Loss: 2.1774325370788574, Accuracy: 24.549999237060547, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.2333498\n",
      "2.1797075\n",
      "2.168501\n",
      "2.1555698\n",
      "2.15449\n",
      "2.144766\n",
      "2.1380272\n",
      "2.137503\n",
      "Epoch 7, Loss: 2.138050079345703, Accuracy: 26.21666717529297, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.2241712\n",
      "2.1323094\n",
      "2.1049438\n",
      "2.0960763\n",
      "2.0891013\n",
      "2.0839942\n",
      "2.081504\n",
      "2.085018\n",
      "Epoch 8, Loss: 2.0877392292022705, Accuracy: 27.883333206176758, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.0380309\n",
      "1.9930614\n",
      "1.9997195\n",
      "2.00407\n",
      "2.0040514\n",
      "2.0211756\n",
      "2.0354204\n",
      "2.0436113\n",
      "Epoch 9, Loss: 2.047916889190674, Accuracy: 29.94999885559082, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "1.5263419\n",
      "1.9696001\n",
      "1.9542879\n",
      "1.9569795\n",
      "1.9679618\n",
      "1.9723545\n",
      "1.9833218\n",
      "1.9902244\n",
      "Epoch 10, Loss: 1.9878935813903809, Accuracy: 31.799999237060547, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "1.8766726\n",
      "1.8457322\n",
      "1.8806506\n",
      "1.895336\n",
      "1.8870198\n",
      "1.901776\n",
      "1.9108888\n",
      "1.9143591\n",
      "Epoch 11, Loss: 1.918094515800476, Accuracy: 34.70000076293945, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "1.6755676\n",
      "1.7793794\n",
      "1.8448426\n",
      "1.8384091\n",
      "1.8541489\n",
      "1.8600731\n",
      "1.8541161\n",
      "1.857201\n",
      "Epoch 12, Loss: 1.8576418161392212, Accuracy: 37.349998474121094, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "2.0494\n",
      "1.880217\n",
      "1.8276453\n",
      "1.7880559\n",
      "1.775765\n",
      "1.7755009\n",
      "1.7738354\n",
      "1.7718017\n",
      "Epoch 13, Loss: 1.777726411819458, Accuracy: 39.11666488647461, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "1.6960883\n",
      "1.6553286\n",
      "1.6828564\n",
      "1.6932124\n",
      "1.7057408\n",
      "1.6936167\n",
      "1.6914161\n",
      "1.6980449\n",
      "Epoch 14, Loss: 1.6980903148651123, Accuracy: 41.766666412353516, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "1.3088411\n",
      "1.414196\n",
      "1.5023228\n",
      "1.5257574\n",
      "1.5443989\n",
      "1.5698105\n",
      "1.5733199\n",
      "1.5881473\n",
      "Epoch 15, Loss: 1.5922825336456299, Accuracy: 45.516666412353516, Test Loss: 0.0, Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "train_data_source = tf.data.Dataset.from_tensor_slices((images, labels)).shuffle(6000).batch(16)\n",
    "# test_data_source = tf.data.Dataset.from_tensor_slices((x_test_source, y_test_source)).batch(32)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "#     test_loss.reset_states()\n",
    "#     test_accuracy.reset_states()\n",
    "\n",
    "    i = 0\n",
    "    curr_losses = []\n",
    "    \n",
    "    for images, labels in train_data_source:\n",
    "        train_step(images, labels)\n",
    "        curr_losses.append(train_loss.result())\n",
    "        \n",
    "        if not i % 50:\n",
    "            print(np.mean(curr_losses))\n",
    "            curr_losses = []\n",
    "            \n",
    "        i += 1\n",
    "\n",
    "#     for test_images, test_labels in test_data_source:\n",
    "#         test_step(test_images, test_labels)\n",
    "\n",
    "    print(\n",
    "        f'Epoch {epoch + 1}, '\n",
    "        f'Loss: {train_loss.result()}, '\n",
    "        f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "        f'Test Loss: {test_loss.result()}, '\n",
    "        f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
